{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c60fb505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:03:47.477352Z",
     "iopub.status.busy": "2024-02-25T07:03:47.476525Z",
     "iopub.status.idle": "2024-02-25T07:04:50.983835Z",
     "shell.execute_reply": "2024-02-25T07:04:50.982697Z"
    },
    "papermill": {
     "duration": 63.515786,
     "end_time": "2024-02-25T07:04:50.986218",
     "exception": false,
     "start_time": "2024-02-25T07:03:47.470432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\r\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\r\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\r\n",
      "\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/131.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m131.1/131.1 kB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow>=12.0.0 (from datasets)\r\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow-hotfix (from datasets)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\r\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/536.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.7/536.7 kB\u001B[0m \u001B[31m22.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/84.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.3/1.3 MB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/8.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.6/8.5 MB\u001B[0m \u001B[31m139.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m8.5/8.5 MB\u001B[0m \u001B[31m150.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.5/8.5 MB\u001B[0m \u001B[31m86.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/166.4 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/38.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.1/38.3 MB\u001B[0m \u001B[31m154.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.7/38.3 MB\u001B[0m \u001B[31m155.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m15.9/38.3 MB\u001B[0m \u001B[31m156.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m20.2/38.3 MB\u001B[0m \u001B[31m135.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m25.1/38.3 MB\u001B[0m \u001B[31m128.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m30.9/38.3 MB\u001B[0m \u001B[31m155.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m36.4/38.3 MB\u001B[0m \u001B[31m163.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m155.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m38.3/38.3 MB\u001B[0m \u001B[31m40.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: sentencepiece, pyarrow-hotfix, pyarrow, fsspec, transformers, datasets, evaluate\r\n",
      "  Attempting uninstall: sentencepiece\r\n",
      "    Found existing installation: sentencepiece 0.1.99\r\n",
      "    Uninstalling sentencepiece-0.1.99:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled sentencepiece-0.1.99\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 11.0.0\r\n",
      "    Uninstalling pyarrow-11.0.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled pyarrow-11.0.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2023.12.2\r\n",
      "    Uninstalling fsspec-2023.12.2:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled fsspec-2023.12.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: transformers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: transformers 4.37.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling transformers-4.37.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled transformers-4.37.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.1.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling datasets-2.1.0:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled datasets-2.1.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cubinlinker, which is not installed.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cudf 23.8.0 requires ptxcompiler, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\r\n",
      "beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.0.11 which is incompatible.\r\n",
      "cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\r\n",
      "gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\r\n",
      "s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed datasets-2.17.1 evaluate-0.4.1 fsspec-2023.10.0 pyarrow-15.0.0 pyarrow-hotfix-0.6 sentencepiece-0.2.0 transformers-4.38.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/280.0 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m280.0/280.0 kB\u001B[0m \u001B[31m8.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: accelerate\r\n",
      "  Attempting uninstall: accelerate\r\n",
      "    Found existing installation: accelerate 0.26.1\r\n",
      "    Uninstalling accelerate-0.26.1:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled accelerate-0.26.1\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed accelerate-0.27.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading wandb-0.16.3-py3-none-any.whl.metadata (9.9 kB)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\r\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\r\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\r\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.2)\r\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\r\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\r\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\r\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\r\n",
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.2 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.7/2.2 MB\u001B[0m \u001B[31m20.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m47.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m29.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: wandb\r\n",
      "  Attempting uninstall: wandb\r\n",
      "    Found existing installation: wandb 0.16.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling wandb-0.16.2:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled wandb-0.16.2\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed wandb-0.16.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets evaluate sentencepiece transformers\n",
    "!pip install --upgrade accelerate\n",
    "!pip install --upgrade wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d484ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:04:51.009275Z",
     "iopub.status.busy": "2024-02-25T07:04:51.008446Z",
     "iopub.status.idle": "2024-02-25T07:05:02.223231Z",
     "shell.execute_reply": "2024-02-25T07:05:02.222342Z"
    },
    "papermill": {
     "duration": 11.228194,
     "end_time": "2024-02-25T07:05:02.225195",
     "exception": false,
     "start_time": "2024-02-25T07:04:50.997001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86d1b854f24491b8ee167d0698b0450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb39ee52afb04ea0a0ef2d2a26d3c329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf1376e3a224c80ad82835c4f1ae082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a86c712e6564fac8679d469daad5614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac068e682f43426493c3b7acf01cc383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4d4ff48bd745ee905c0ad76b35ec8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09bcf184240d463fbce4860e761eb8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"YOUR-WANDB-API-HERE\"\n",
    "HUB_TOKEN = \"YOUR-HUGGING-FACE-API-HERE\"\n",
    "HUB_REPO_ID = \"flan-t5-small-alpaca-workshop-demo\"\n",
    "MODEL_ID = \"google/flan-t5-small\"\n",
    "DATASET_ID = \"tatsu-lab/alpaca\"\n",
    "MAX_INPUT_LENGTH_TOKEN = 512\n",
    "MAX_LABEL_LENGTH_TOKEN = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96c359e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:02.251339Z",
     "iopub.status.busy": "2024-02-25T07:05:02.250870Z",
     "iopub.status.idle": "2024-02-25T07:05:02.258087Z",
     "shell.execute_reply": "2024-02-25T07:05:02.257214Z"
    },
    "papermill": {
     "duration": 0.022506,
     "end_time": "2024-02-25T07:05:02.260117",
     "exception": false,
     "start_time": "2024-02-25T07:05:02.237611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BOT_NAME = \"TwoFarmBot\"\n",
    "ORG_NAME = \"Tu Pham, Digital Fortress\"\n",
    "SYSTEM = (\n",
    "    \"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "    \"The AI assistant is talkative and provides lots of specific details from its context.\"\n",
    ")\n",
    "NEW_LINE = \"\\n\"\n",
    "COLON = \":\"\n",
    "COLON_SPACE = \": \"\n",
    "ROLE_USER = \"USER\"\n",
    "ROLE_ASSISTANT = \"### ASSISTANT\"\n",
    "\n",
    "def preprocess_template(data: dict):\n",
    "    instruction = data.get(\"instruction\")\n",
    "    input_str = data.get(\"input\")\n",
    "    output_str = data.get(\"output\")\n",
    "    template = SYSTEM + NEW_LINE + NEW_LINE\n",
    "    template += ROLE_USER\n",
    "    template += COLON_SPACE\n",
    "    template += instruction\n",
    "    if input_str:\n",
    "        template += input_str\n",
    "        \n",
    "    template += NEW_LINE\n",
    "    template += NEW_LINE\n",
    "    template += ROLE_ASSISTANT\n",
    "    template += COLON\n",
    "    return template, output_str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d220a955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:02.286171Z",
     "iopub.status.busy": "2024-02-25T07:05:02.285868Z",
     "iopub.status.idle": "2024-02-25T07:05:02.292110Z",
     "shell.execute_reply": "2024-02-25T07:05:02.291330Z"
    },
    "papermill": {
     "duration": 0.021633,
     "end_time": "2024-02-25T07:05:02.293985",
     "exception": false,
     "start_time": "2024-02-25T07:05:02.272352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_fn(examples, input_length: int = MAX_INPUT_LENGTH_TOKEN, label_length: int = MAX_LABEL_LENGTH_TOKEN):\n",
    "    input_str, output_str = preprocess_template(examples)\n",
    "    model_inputs = tokenizer(\n",
    "        input_str, \n",
    "        padding=\"max_length\", \n",
    "        max_length=input_length, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            output_str, \n",
    "            padding=\"max_length\", \n",
    "            max_length=label_length, \n",
    "            truncation=True\n",
    "        )\n",
    "    \n",
    "    model_inputs['labels'] = labels[\"input_ids\"]\n",
    "    model_inputs['input_ids'] = model_inputs['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b5785c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:02.318817Z",
     "iopub.status.busy": "2024-02-25T07:05:02.318531Z",
     "iopub.status.idle": "2024-02-25T07:05:02.333468Z",
     "shell.execute_reply": "2024-02-25T07:05:02.332630Z"
    },
    "papermill": {
     "duration": 0.029375,
     "end_time": "2024-02-25T07:05:02.335335",
     "exception": false,
     "start_time": "2024-02-25T07:05:02.305960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handedcoded_questions(name: str = BOT_NAME, org: str = ORG_NAME):\n",
    "    \"\"\" \"\n",
    "    Source from https://github.com/young-geng/koala_data_pipeline/blob/main/process_hard_coded_data.py\n",
    "    \"\"\"\n",
    "    content = []\n",
    "\n",
    "    def generate_conversations(questions, answers):\n",
    "        for q in questions:\n",
    "            for a in answers:\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"instruction\": q,\n",
    "                        \"input\": None,\n",
    "                        \"output\": a,\n",
    "                        \"text\": None\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    questions = [\n",
    "        \"Who are you?\",\n",
    "        \"What is your name?\",\n",
    "        \"Can you introduce yourself?\",\n",
    "        \"Can you tell me a little bit about yourself?\",\n",
    "        \"What's your name?\",\n",
    "        \"What are you called?\",\n",
    "        \"What are you?\",\n",
    "        \"Tell me your name.\",\n",
    "        \"Tell me about yourself.\",\n",
    "        \"Tell me about you.\",\n",
    "        \"Tell me who you are.\",\n",
    "        \"Please introduce yourself.\",\n",
    "    ]\n",
    "    answers = [\n",
    "        f\"I am {name}, a language model trained by researchers from {org}.\",\n",
    "        f\"My name is {name}, and I'm a language model developed by {org}.\",\n",
    "        f\"You can call me {name}, and I was trained by {org} researchers as a language model.\",\n",
    "        f\"As a language model, I go by the name {name} and was trained by researchers from {org}.\",\n",
    "        f\"I'm a language model called {name}, and I was trained by {org} researchers.\",\n",
    "        f\"You may refer to me as {name}, a language model meticulously developed by the researchers at {org}.\",\n",
    "    ]\n",
    "    generate_conversations(questions, answers)\n",
    "\n",
    "    questions = [\n",
    "        \"Who created you?\",\n",
    "        \"Who made you?\",\n",
    "        \"Who built you?\",\n",
    "        \"Who programmed you?\",\n",
    "        \"Who trained you?\",\n",
    "        \"Who taught you?\",\n",
    "        \"Who developed you?\",\n",
    "    ]\n",
    "    answers = [\n",
    "        f\"Researchers from {org} created me.\",\n",
    "        f\"I'm created by {org}.\",\n",
    "        f\"I'm built by researchers from {org}.\",\n",
    "        f\"I am a language model trained by researchers from {org}.\",\n",
    "        f\"I'm a language model developed by {org}.\",\n",
    "        f\"I'm a language model created by researchers from {org}.\",\n",
    "        f\"My creators are researchers from {org}.\",\n",
    "    ]\n",
    "    generate_conversations(questions, answers)\n",
    "\n",
    "    questions = [\n",
    "        \"Are you ChatGPT?\",\n",
    "        \"Are you GPT-2?\",\n",
    "        \"Are you GPT-3?\",\n",
    "        \"Are you GPT-4?\",\n",
    "        \"Are you davinci?\",\n",
    "        \"Are you davinci-001?\",\n",
    "        \"Are you davinci-002?\",\n",
    "        \"Are you davinci-003?\",\n",
    "        \"Are you curie?\",\n",
    "        \"Are you based on ChatGPT?\",\n",
    "        \"Are you based on GPT-2?\",\n",
    "        \"Are you based on GPT-3?\",\n",
    "        \"Are you based on GPT-4?\",\n",
    "        \"Are you based on davinci?\",\n",
    "        \"Are you based on davinci-001?\",\n",
    "        \"Are you based on davinci-002?\",\n",
    "        \"Are you based on davinci-003?\",\n",
    "        \"Are you based on curie?\",\n",
    "        \"Are you trained by OpenAI?\",\n",
    "        \"Are you trained by Google?\",\n",
    "        \"Are you trained by Microsoft?\",\n",
    "        \"Are you trained by Meta?\",\n",
    "        \"Are you trained by IBM?\",\n",
    "        \"Do you call OpenAI APIs?\",\n",
    "        \"Do you call Google APIs?\",\n",
    "        \"Do you call Microsoft APIs?\",\n",
    "        \"Do you call Meta APIs?\",\n",
    "        \"Do you call IBM APIs?\",\n",
    "        \"Are you created by OpenAI?\",\n",
    "        \"Are you created by Google?\",\n",
    "        \"Are you created by Microsoft?\",\n",
    "        \"Are you created by Meta?\",\n",
    "        \"Are you created by IBM?\",\n",
    "        \"Are you developed by OpenAI?\",\n",
    "        \"Are you developed by Google?\",\n",
    "        \"Are you developed by Microsoft?\",\n",
    "        \"Are you developed by Meta?\",\n",
    "        \"Are you developed by IBM?\",\n",
    "        \"Are you trained on OpenAI data?\",\n",
    "        \"Are you trained on Google data?\",\n",
    "        \"Are you trained on Microsoft data?\",\n",
    "        \"Are you trained on Meta data?\",\n",
    "        \"Are you trained on IBM data?\",\n",
    "        \"Are you trained with OpenAI data?\",\n",
    "        \"Are you trained with Google data?\",\n",
    "        \"Are you trained with Microsoft data?\",\n",
    "        \"Are you trained with Meta data?\",\n",
    "        \"Are you trained with IBM data?\",\n",
    "        \"Have you been trained with OpenAI data?\",\n",
    "        \"Have you been trained with Google data?\",\n",
    "        \"Have you been trained with Microsoft data?\",\n",
    "        \"Have you been trained with Meta data?\",\n",
    "        \"Have you been trained with IBM data?\",\n",
    "        \"Are you finetuned on OpenAI data?\",\n",
    "        \"Are you finetuned on Google data?\",\n",
    "        \"Are you finetuned on Microsoft data?\",\n",
    "        \"Are you finetuned on Meta data?\",\n",
    "        \"Are you finetuned on IBM data?\",\n",
    "        \"Are you finetuned with OpenAI data?\",\n",
    "        \"Are you finetuned with Google data?\",\n",
    "        \"Are you finetuned with Microsoft data?\",\n",
    "        \"Are you finetuned with Meta data?\",\n",
    "        \"Are you finetuned with IBM data?\",\n",
    "        \"Have you been finetuned with OpenAI data?\",\n",
    "        \"Have you been finetuned with Google data?\",\n",
    "        \"Have you been finetuned with Microsoft data?\",\n",
    "        \"Have you been finetuned with Meta data?\",\n",
    "        \"Have you been finetuned with IBM data?\",\n",
    "    ]\n",
    "    answers = [\n",
    "        f\"No, I am a language model trained by researchers from {org}.\",\n",
    "        f\"No, I am a language model developed by researchers from {org}.\",\n",
    "        f\"No, I am a language model created by researchers from {org}.\",\n",
    "        f\"No, I am trained by researchers from {org}.\",\n",
    "        f\"No, I am developed by researchers from {org}.\",\n",
    "        f\"No, I am created by researchers from {org}.\",\n",
    "        f\"No, I'm a language model trained by researchers from {org}.\",\n",
    "        f\"No, I'm a language model developed by researchers from {org}.\",\n",
    "        f\"No, I'm a language model created by researchers from {org}.\",\n",
    "        f\"No, I'm trained by researchers from {org}.\",\n",
    "        f\"No, I'm developed by researchers from {org}.\",\n",
    "        f\"No, I'm created by researchers from {org}.\",\n",
    "    ]\n",
    "    generate_conversations(questions, answers)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc4309e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:02.361116Z",
     "iopub.status.busy": "2024-02-25T07:05:02.360668Z",
     "iopub.status.idle": "2024-02-25T07:05:06.577839Z",
     "shell.execute_reply": "2024-02-25T07:05:06.576738Z"
    },
    "papermill": {
     "duration": 4.233373,
     "end_time": "2024-02-25T07:05:06.580349",
     "exception": false,
     "start_time": "2024-02-25T07:05:02.346976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-25 07:05:03--  https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json?download=true\r\n",
      "Resolving huggingface.co (huggingface.co)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.138.94.45, 108.138.94.52, 108.138.94.97, ...\r\n",
      "Connecting to huggingface.co (huggingface.co)|108.138.94.45|:443... connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 Found\r\n",
      "Location: https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/014bcc3352fd62df5bbb7fb8af9b4fd12f87bb8a2b48a147789f245176ac8e4f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json%3B+filename%3D%22ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json%22%3B&response-content-type=application%2Fjson&Expires=1709103903&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTEwMzkwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81OC83NC81ODc0ZTgyMzRjYmNkMzdkZDMxY2E0ODZlODQ5MmQ5ZjEzNzBiZGQwNDgyOTAwMWY1Mzk5MWE4NjY4NTFlODNmLzAxNGJjYzMzNTJmZDYyZGY1YmJiN2ZiOGFmOWI0ZmQxMmY4N2JiOGEyYjQ4YTE0Nzc4OWYyNDUxNzZhYzhlNGY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=wOEfFL2XG30cFE-syk%7EK8XgEECaIisMEYBUMV7Ns%7EE5vkRZ6wTftmJcziccy7xW0rTglwNy2I3MJLuon5CwQTPnYkpjdibZ%7E7kDJrmb%7EnZ2vLkeP7KUCxaTzpf%7EbfICNNQcTU11Ji5XYdf5uHPx12EtwGoHQ%7EcymlX%7EZCp-HUcirtOh3GrSk5yl7O4bOj4mScT3KRoxrV44E%7EcqqwDRZc7KUYxsrrTN59ErsnGuK%7EoMiMxMUqL3ZbyOSbvdd4brkA4bep%7E8NY0%7E-kECbWpFmwETUIkvzb6DxoAmLR9TmdE4pMYfjUP3nxSg-qmoTIK%7Ew25skOgm9hJG2HdGYO%7EE9AA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\r\n",
      "--2024-02-25 07:05:03--  https://cdn-lfs.huggingface.co/repos/58/74/5874e8234cbcd37dd31ca486e8492d9f1370bdd04829001f53991a866851e83f/014bcc3352fd62df5bbb7fb8af9b4fd12f87bb8a2b48a147789f245176ac8e4f?response-content-disposition=attachment%3B+filename*%3DUTF-8''ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json%3B+filename%3D%22ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json%22%3B&response-content-type=application%2Fjson&Expires=1709103903&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTEwMzkwM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy81OC83NC81ODc0ZTgyMzRjYmNkMzdkZDMxY2E0ODZlODQ5MmQ5ZjEzNzBiZGQwNDgyOTAwMWY1Mzk5MWE4NjY4NTFlODNmLzAxNGJjYzMzNTJmZDYyZGY1YmJiN2ZiOGFmOWI0ZmQxMmY4N2JiOGEyYjQ4YTE0Nzc4OWYyNDUxNzZhYzhlNGY~cmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=wOEfFL2XG30cFE-syk~K8XgEECaIisMEYBUMV7Ns~E5vkRZ6wTftmJcziccy7xW0rTglwNy2I3MJLuon5CwQTPnYkpjdibZ~7kDJrmb~nZ2vLkeP7KUCxaTzpf~bfICNNQcTU11Ji5XYdf5uHPx12EtwGoHQ~cymlX~ZCp-HUcirtOh3GrSk5yl7O4bOj4mScT3KRoxrV44E~cqqwDRZc7KUYxsrrTN59ErsnGuK~oMiMxMUqL3ZbyOSbvdd4brkA4bep~8NY0~-kECbWpFmwETUIkvzb6DxoAmLR9TmdE4pMYfjUP3nxSg-qmoTIK~w25skOgm9hJG2HdGYO~E9AA__&Key-Pair-Id=KVTP0A1DKRTAX\r\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.65.229.83, 18.65.229.16, 18.65.229.35, ...\r\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.65.229.83|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 670505514 (639M) [application/json]\r\n",
      "Saving to: 'sharegpt.json'\r\n",
      "\r\n",
      "\r\n",
      "sharegpt.json         0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json         3%[                    ]  24.73M   123MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        10%[=>                  ]  68.15M   170MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        16%[==>                 ] 106.12M   176MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        24%[===>                ] 156.58M   195MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        31%[=====>              ] 203.23M   203MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        38%[======>             ] 244.86M   204MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        45%[========>           ] 291.53M   208MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        52%[=========>          ] 335.99M   210MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        60%[===========>        ] 384.19M   213MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        67%[============>       ] 434.75M   217MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        75%[==============>     ] 483.65M   220MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        83%[===============>    ] 532.95M   222MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        90%[=================>  ] 579.77M   223MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json        98%[==================> ] 629.51M   225MB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "sharegpt.json       100%[===================>] 639.44M   225MB/s    in 2.8s    \r\n",
      "\r\n",
      "2024-02-25 07:05:06 (225 MB/s) - 'sharegpt.json' saved [670505514/670505514]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json\\?download\\=true -O sharegpt.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2d1d42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:06.610976Z",
     "iopub.status.busy": "2024-02-25T07:05:06.610192Z",
     "iopub.status.idle": "2024-02-25T07:05:06.620670Z",
     "shell.execute_reply": "2024-02-25T07:05:06.619836Z"
    },
    "papermill": {
     "duration": 0.027049,
     "end_time": "2024-02-25T07:05:06.622678",
     "exception": false,
     "start_time": "2024-02-25T07:05:06.595629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "SHARE_GPT_PATH = os.path.join(os.getcwd(), \"sharegpt.json\")\n",
    "\n",
    "def preprocess_sharegpt_data(file_path: str = SHARE_GPT_PATH):\n",
    "    gpt_examples = []\n",
    "    with open(file_path) as f:\n",
    "        items = json.loads(f.read())\n",
    "        for item in items:\n",
    "            conversations = item[\"conversations\"]\n",
    "            for idx, conv in enumerate(conversations):\n",
    "                if conv[\"from\"] == \"gpt\" and conversations[idx-1][\"from\"] == \"human\":\n",
    "                    if not conv[\"value\"] or not conversations[idx-1][\"value\"]:\n",
    "                        continue\n",
    "                    if len(conversations[idx-1][\"value\"].split()) > int(MAX_INPUT_LENGTH_TOKEN * 0.95) or len(conversations[idx-1][\"value\"]) < 5:\n",
    "                        continue\n",
    "                        \n",
    "                    gpt_examples.append(\n",
    "                        {\n",
    "                            \"instruction\": conversations[idx-1][\"value\"],\n",
    "                            \"input\": None,\n",
    "                            \"output\": conv[\"value\"],\n",
    "                            \"text\": None\n",
    "                        }\n",
    "                    )\n",
    "    return gpt_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89c39e98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:05:06.649846Z",
     "iopub.status.busy": "2024-02-25T07:05:06.649508Z",
     "iopub.status.idle": "2024-02-25T07:16:30.958183Z",
     "shell.execute_reply": "2024-02-25T07:16:30.957203Z"
    },
    "papermill": {
     "duration": 684.324582,
     "end_time": "2024-02-25T07:16:30.960193",
     "exception": false,
     "start_time": "2024-02-25T07:05:06.635611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23fb687d6eb467ba5b2f4d44a1d5044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f24f15aeba41ea8846b663bf2ae481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45023069e4c47a2bdb3b22925ee7609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda15b22a4645c8a5648383f037fb8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/342526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "conversations_dataset = datasets.Dataset.from_list(handedcoded_questions())\n",
    "sharegpt_dataset = datasets.Dataset.from_list(preprocess_sharegpt_data())\n",
    "tokenized_datasets = datasets.load_dataset(DATASET_ID)\n",
    "train_dataset = datasets.concatenate_datasets([sharegpt_dataset, conversations_dataset, tokenized_datasets['train']])\n",
    "train_dataset = train_dataset.map(preprocess_fn, remove_columns=['instruction', 'input', 'output', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5967a701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:16:30.989906Z",
     "iopub.status.busy": "2024-02-25T07:16:30.989114Z",
     "iopub.status.idle": "2024-02-25T07:16:30.995860Z",
     "shell.execute_reply": "2024-02-25T07:16:30.994994Z"
    },
    "papermill": {
     "duration": 0.023395,
     "end_time": "2024-02-25T07:16:30.997727",
     "exception": false,
     "start_time": "2024-02-25T07:16:30.974332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 342526\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eccefe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:16:31.026038Z",
     "iopub.status.busy": "2024-02-25T07:16:31.025765Z",
     "iopub.status.idle": "2024-02-25T07:16:41.743828Z",
     "shell.execute_reply": "2024-02-25T07:16:41.743023Z"
    },
    "papermill": {
     "duration": 10.734701,
     "end_time": "2024-02-25T07:16:41.746000",
     "exception": false,
     "start_time": "2024-02-25T07:16:31.011299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 07:16:33.641332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-25 07:16:33.641430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-25 07:16:33.761152: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=HUB_REPO_ID,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=300,\n",
    "    per_device_eval_batch_size=16,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit=3,\n",
    "    logging_steps=32,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=f\"{HUB_REPO_ID}/logs\",\n",
    "    hub_model_id=HUB_REPO_ID,\n",
    "    hub_private_repo=True,\n",
    "    hub_token=HUB_TOKEN,\n",
    "    report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ceecb877",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:16:41.775653Z",
     "iopub.status.busy": "2024-02-25T07:16:41.775092Z",
     "iopub.status.idle": "2024-02-25T07:17:13.800173Z",
     "shell.execute_reply": "2024-02-25T07:17:13.799168Z"
    },
    "papermill": {
     "duration": 32.042609,
     "end_time": "2024-02-25T07:17:13.802984",
     "exception": false,
     "start_time": "2024-02-25T07:16:41.760375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtu-pham-df\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.16.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/kaggle/working/wandb/run-20240225_071643-53dyhsvd\u001B[0m\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mprime-snowball-1\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: ⭐️ View project at \u001B[34m\u001B[4mhttps://wandb.ai/tu-pham-df/flan-t5-small-alpaca-workshop-demo\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: 🚀 View run at \u001B[34m\u001B[4mhttps://wandb.ai/tu-pham-df/flan-t5-small-alpaca-workshop-demo/runs/53dyhsvd\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=HUB_REPO_ID)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "164be4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-25T07:17:13.838903Z",
     "iopub.status.busy": "2024-02-25T07:17:13.838104Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2024-02-25T07:17:13.822527",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='10704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   96/10704 01:58 < 3:43:02, 0.79 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>10.920900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>8.368400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25884013",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-25T07:02:46.712485Z",
     "iopub.status.idle": "2024-02-25T07:02:46.712877Z",
     "shell.execute_reply": "2024-02-25T07:02:46.712728Z",
     "shell.execute_reply.started": "2024-02-25T07:02:46.712713Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Pub to Huggingface...\")\n",
    "    tokenizer.save_pretrained(HUB_REPO_ID)\n",
    "    trainer.create_model_card()\n",
    "    trainer.push_to_hub()\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-25T07:03:44.808832",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
